{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MINERÍA DE DATOS - Modelo Predictivo de Enfermedades Cardíacas\n",
        "\n",
        "## Contenido:\n",
        "1. Carga y Preparación de Datos\n",
        "2. Análisis Exploratorio\n",
        "3. Feature Engineering\n",
        "4. Entrenamiento de Modelos (5 algoritmos)\n",
        "5. Evaluación y Comparación\n",
        "6. Hiperparametrización del Mejor Modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. CARGA Y PREPARACIÓN DE DATOS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
        "                             confusion_matrix, classification_report, roc_curve, auc, roc_auc_score)\n",
        "\n",
        "# Modelos\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar datos:\n",
        "df = pd.read_csv('heart_disease_clean.csv')\n",
        "print(f\"Dataset cargado: {df.shape}\")\n",
        "print(f\"Registros: {df.shape[0]}, Variables: {df.shape[1]}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Información del dataset:\n",
        "print(\"Información del Dataset:\")\n",
        "print(df.info())\n",
        "print(\"\\nVerificación de valores faltantes:\")\n",
        "print(df.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ANÁLISIS EXPLORATORIO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribución de la variable target:\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "df['heart_disease'].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\n",
        "plt.title('Distribución de Enfermedades Cardíacas')\n",
        "plt.xlabel('Heart Disease')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.xticks([0, 1], ['No', 'Sí'], rotation=0)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "df['heart_disease'].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=['skyblue', 'salmon'])\n",
        "plt.title('Porcentaje de Enfermedades Cardíacas')\n",
        "plt.ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nDistribución de la variable objetivo:\")\n",
        "print(df['heart_disease'].value_counts())\n",
        "print(\"\\nBalance del dataset:\")\n",
        "print(df['heart_disease'].value_counts(normalize=True) * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlación con la variable target:\n",
        "numeric_cols = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak', 'ca']\n",
        "correlations = df[numeric_cols + ['heart_disease']].corr()['heart_disease'].drop('heart_disease').sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "correlations.plot(kind='barh', color='steelblue')\n",
        "plt.title('Correlación de Variables Numéricas con Enfermedad Cardíaca')\n",
        "plt.xlabel('Correlación')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCorrelaciones con heart_disease:\")\n",
        "print(correlations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. FEATURE ENGINEERING Y PREPARACIÓN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Codificar variables categóricas:\n",
        "df_model = df.copy()\n",
        "\n",
        "# Label Encoding para variables categóricas:\n",
        "label_encoders = {}\n",
        "categorical_columns = ['sex', 'cp', 'restecg', 'slope', 'thal', 'dataset']\n",
        "\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    df_model[col] = le.fit_transform(df_model[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "    print(f\"{col} codificado\")\n",
        "\n",
        "# Convertir booleanos a numérico\n",
        "df_model['fbs'] = df_model['fbs'].astype(int)\n",
        "df_model['exang'] = df_model['exang'].astype(int)\n",
        "\n",
        "print(\"\\nCodificación completada\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar features y target:\n",
        "# Excluir columnas no necesarias para el modelo:\n",
        "columns_to_exclude = ['id', 'num', 'heart_disease']\n",
        "feature_columns = [col for col in df_model.columns if col not in columns_to_exclude]\n",
        "\n",
        "X = df_model[feature_columns]\n",
        "y = df_model['heart_disease']\n",
        "\n",
        "print(f\"Features seleccionadas: {X.shape[1]}\")\n",
        "print(f\"Columnas: {list(X.columns)}\")\n",
        "print(f\"\\nTarget: {y.name}\")\n",
        "print(f\"Total de registros: {len(X)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# División de datos: 70% train, 15% validation, 15% test:\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(\"División de datos:\")\n",
        "print(f\"  Train:      {X_train.shape[0]} registros ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  Validation: {X_val.shape[0]} registros ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  Test:       {X_test.shape[0]} registros ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nDistribución de clases en cada conjunto:\")\n",
        "print(f\"  Train:      {y_train.value_counts().to_dict()}\")\n",
        "print(f\"  Validation: {y_val.value_counts().to_dict()}\")\n",
        "print(f\"  Test:       {y_test.value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalización de features:\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Normalización completada\")\n",
        "print(f\"Media de features escaladas: {X_train_scaled.mean():.4f}\")\n",
        "print(f\"Desviación estándar: {X_train_scaled.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ENTRENAMIENTO DE MODELOS\n",
        "\n",
        "Vamos a entrenar y evaluar 5 algoritmos diferentes:\n",
        "1. **Árbol de Decisión** (Decision Tree)\n",
        "2. **K-Nearest Neighbors** (KNN)\n",
        "3. **Red Neuronal** (MLP)\n",
        "4. **Support Vector Machine** (SVM)\n",
        "5. **Random Forest**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para evaluar modelos:\n",
        "def evaluate_model(name, model, X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Evalúa un modelo y retorna métricas\"\"\"\n",
        "    # Entrenar:\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Predecir:\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_val = model.predict(X_val)\n",
        "    \n",
        "    # Calcular métricas:\n",
        "    metrics = {\n",
        "        'Modelo': name,\n",
        "        'Accuracy_Train': accuracy_score(y_train, y_pred_train),\n",
        "        'Accuracy_Val': accuracy_score(y_val, y_pred_val),\n",
        "        'Precision_Val': precision_score(y_val, y_pred_val),\n",
        "        'Recall_Val': recall_score(y_val, y_pred_val),\n",
        "        'F1_Score_Val': f1_score(y_val, y_pred_val),\n",
        "        'ROC_AUC_Val': roc_auc_score(y_val, y_pred_val)\n",
        "    }\n",
        "    \n",
        "    return metrics, model\n",
        "\n",
        "# Diccionario para almacenar resultados:\n",
        "results = []\n",
        "trained_models = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n1. ÁRBOL DE DECISIÓN (Decision Tree)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "dt_model = DecisionTreeClassifier(\n",
        "    criterion='gini',\n",
        "    max_depth=10,\n",
        "    min_samples_split=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "dt_metrics, dt_trained = evaluate_model('Decision Tree', dt_model, X_train_scaled, y_train, X_val_scaled, y_val)\n",
        "results.append(dt_metrics)\n",
        "trained_models['Decision Tree'] = dt_trained\n",
        "\n",
        "print(f\"\\nAccuracy Train: {dt_metrics['Accuracy_Train']:.4f}\")\n",
        "print(f\"Accuracy Val:   {dt_metrics['Accuracy_Val']:.4f}\")\n",
        "print(f\"Precision Val:  {dt_metrics['Precision_Val']:.4f}\")\n",
        "print(f\"Recall Val:     {dt_metrics['Recall_Val']:.4f}\")\n",
        "print(f\"F1-Score Val:   {dt_metrics['F1_Score_Val']:.4f}\")\n",
        "print(f\"ROC-AUC Val:    {dt_metrics['ROC_AUC_Val']:.4f}\")\n",
        "\n",
        "# Matriz de confusión:\n",
        "y_pred_dt = dt_trained.predict(X_val_scaled)\n",
        "cm = confusion_matrix(y_val, y_pred_dt)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Matriz de Confusión - Decision Tree')\n",
        "plt.ylabel('Real')\n",
        "plt.xlabel('Predicción')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nReporte de Clasificación:\")\n",
        "print(classification_report(y_val, y_pred_dt, target_names=['No Disease', 'Disease']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n2. K-NEAREST NEIGHBORS (KNN)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "knn_model = KNeighborsClassifier(\n",
        "    n_neighbors=5,\n",
        "    metric='euclidean',\n",
        "    weights='distance'\n",
        ")\n",
        "\n",
        "knn_metrics, knn_trained = evaluate_model('KNN', knn_model, X_train_scaled, y_train, X_val_scaled, y_val)\n",
        "results.append(knn_metrics)\n",
        "trained_models['KNN'] = knn_trained\n",
        "\n",
        "print(f\"\\nAccuracy Train: {knn_metrics['Accuracy_Train']:.4f}\")\n",
        "print(f\"Accuracy Val:   {knn_metrics['Accuracy_Val']:.4f}\")\n",
        "print(f\"Precision Val:  {knn_metrics['Precision_Val']:.4f}\")\n",
        "print(f\"Recall Val:     {knn_metrics['Recall_Val']:.4f}\")\n",
        "print(f\"F1-Score Val:   {knn_metrics['F1_Score_Val']:.4f}\")\n",
        "print(f\"ROC-AUC Val:    {knn_metrics['ROC_AUC_Val']:.4f}\")\n",
        "\n",
        "# Matriz de confusión:\n",
        "y_pred_knn = knn_trained.predict(X_val_scaled)\n",
        "cm = confusion_matrix(y_val, y_pred_knn)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', cbar=False)\n",
        "plt.title('Matriz de Confusión - KNN')\n",
        "plt.ylabel('Real')\n",
        "plt.xlabel('Predicción')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nReporte de Clasificación:\")\n",
        "print(classification_report(y_val, y_pred_knn, target_names=['No Disease', 'Disease']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Red Neuronal (Multi-Layer Perceptron)\n",
        "\n",
        "**Configuración del modelo:**\n",
        "- Hidden layers: (100, 50) - dos capas ocultas\n",
        "- Activation: relu\n",
        "- Solver: adam\n",
        "- Max iterations: 500\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n3. RED NEURONAL (Multi-Layer Perceptron)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "mlp_model = MLPClassifier(\n",
        "    hidden_layer_sizes=(100, 50),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=500,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "mlp_metrics, mlp_trained = evaluate_model('Neural Network', mlp_model, X_train_scaled, y_train, X_val_scaled, y_val)\n",
        "results.append(mlp_metrics)\n",
        "trained_models['Neural Network'] = mlp_trained\n",
        "\n",
        "print(f\"\\nAccuracy Train: {mlp_metrics['Accuracy_Train']:.4f}\")\n",
        "print(f\"Accuracy Val:   {mlp_metrics['Accuracy_Val']:.4f}\")\n",
        "print(f\"Precision Val:  {mlp_metrics['Precision_Val']:.4f}\")\n",
        "print(f\"Recall Val:     {mlp_metrics['Recall_Val']:.4f}\")\n",
        "print(f\"F1-Score Val:   {mlp_metrics['F1_Score_Val']:.4f}\")\n",
        "print(f\"ROC-AUC Val:    {mlp_metrics['ROC_AUC_Val']:.4f}\")\n",
        "\n",
        "# Matriz de confusión:\n",
        "y_pred_mlp = mlp_trained.predict(X_val_scaled)\n",
        "cm = confusion_matrix(y_val, y_pred_mlp)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Purples', cbar=False)\n",
        "plt.title('Matriz de Confusión - Neural Network')\n",
        "plt.ylabel('Real')\n",
        "plt.xlabel('Predicción')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nReporte de Clasificación:\")\n",
        "print(classification_report(y_val, y_pred_mlp, target_names=['No Disease', 'Disease']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Support Vector Machine (SVM)\n",
        "\n",
        "**Configuración del modelo:**\n",
        "- Kernel: rbf (Radial Basis Function)\n",
        "- C: 1.0 (parámetro de regularización)\n",
        "- Gamma: scale\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n4. SUPPORT VECTOR MACHINE (SVM)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "svm_model = SVC(\n",
        "    kernel='rbf',\n",
        "    C=1.0,\n",
        "    gamma='scale',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "svm_metrics, svm_trained = evaluate_model('SVM', svm_model, X_train_scaled, y_train, X_val_scaled, y_val)\n",
        "results.append(svm_metrics)\n",
        "trained_models['SVM'] = svm_trained\n",
        "\n",
        "print(f\"\\nAccuracy Train: {svm_metrics['Accuracy_Train']:.4f}\")\n",
        "print(f\"Accuracy Val:   {svm_metrics['Accuracy_Val']:.4f}\")\n",
        "print(f\"Precision Val:  {svm_metrics['Precision_Val']:.4f}\")\n",
        "print(f\"Recall Val:     {svm_metrics['Recall_Val']:.4f}\")\n",
        "print(f\"F1-Score Val:   {svm_metrics['F1_Score_Val']:.4f}\")\n",
        "print(f\"ROC-AUC Val:    {svm_metrics['ROC_AUC_Val']:.4f}\")\n",
        "\n",
        "# Matriz de confusión:\n",
        "y_pred_svm = svm_trained.predict(X_val_scaled)\n",
        "cm = confusion_matrix(y_val, y_pred_svm)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', cbar=False)\n",
        "plt.title('Matriz de Confusión - SVM')\n",
        "plt.ylabel('Real')\n",
        "plt.xlabel('Predicción')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nReporte de Clasificación:\")\n",
        "print(classification_report(y_val, y_pred_svm, target_names=['No Disease', 'Disease']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.5 Random Forest\n",
        "\n",
        "**Configuración del modelo:**\n",
        "- N estimators: 100 árboles\n",
        "- Max depth: 15\n",
        "- Min samples split: 10\n",
        "- Random state: 42\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n5. RANDOM FOREST\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    min_samples_split=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_metrics, rf_trained = evaluate_model('Random Forest', rf_model, X_train_scaled, y_train, X_val_scaled, y_val)\n",
        "results.append(rf_metrics)\n",
        "trained_models['Random Forest'] = rf_trained\n",
        "\n",
        "print(f\"\\nAccuracy Train: {rf_metrics['Accuracy_Train']:.4f}\")\n",
        "print(f\"Accuracy Val:   {rf_metrics['Accuracy_Val']:.4f}\")\n",
        "print(f\"Precision Val:  {rf_metrics['Precision_Val']:.4f}\")\n",
        "print(f\"Recall Val:     {rf_metrics['Recall_Val']:.4f}\")\n",
        "print(f\"F1-Score Val:   {rf_metrics['F1_Score_Val']:.4f}\")\n",
        "print(f\"ROC-AUC Val:    {rf_metrics['ROC_AUC_Val']:.4f}\")\n",
        "\n",
        "# Matriz de confusión:\n",
        "y_pred_rf = rf_trained.predict(X_val_scaled)\n",
        "cm = confusion_matrix(y_val, y_pred_rf)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', cbar=False)\n",
        "plt.title('Matriz de Confusión - Random Forest')\n",
        "plt.ylabel('Real')\n",
        "plt.xlabel('Predicción')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nReporte de Clasificación:\")\n",
        "print(classification_report(y_val, y_pred_rf, target_names=['No Disease', 'Disease']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. COMPARACIÓN DE MODELOS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear DataFrame con resultados:\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.round(4)\n",
        "\n",
        "print(\"\\nCOMPARACIÓN DE MODELOS\")\n",
        "print(\"=\" * 80)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Identificar el mejor modelo:\n",
        "best_model_idx = results_df['F1_Score_Val'].idxmax()\n",
        "best_model_name = results_df.loc[best_model_idx, 'Modelo']\n",
        "print(f\"\\nMEJOR MODELO: {best_model_name}\")\n",
        "print(f\"  F1-Score: {results_df.loc[best_model_idx, 'F1_Score_Val']:.4f}\")\n",
        "print(f\"  Accuracy: {results_df.loc[best_model_idx, 'Accuracy_Val']:.4f}\")\n",
        "print(f\"  ROC-AUC:  {results_df.loc[best_model_idx, 'ROC_AUC_Val']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualización comparativa:\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Accuracy:\n",
        "axes[0, 0].barh(results_df['Modelo'], results_df['Accuracy_Val'], color='steelblue')\n",
        "axes[0, 0].set_xlabel('Accuracy')\n",
        "axes[0, 0].set_title('Accuracy en Validación')\n",
        "axes[0, 0].set_xlim([0, 1])\n",
        "\n",
        "# Precision:\n",
        "axes[0, 1].barh(results_df['Modelo'], results_df['Precision_Val'], color='green')\n",
        "axes[0, 1].set_xlabel('Precision')\n",
        "axes[0, 1].set_title('Precision en Validación')\n",
        "axes[0, 1].set_xlim([0, 1])\n",
        "\n",
        "# Recall:\n",
        "axes[1, 0].barh(results_df['Modelo'], results_df['Recall_Val'], color='orange')\n",
        "axes[1, 0].set_xlabel('Recall')\n",
        "axes[1, 0].set_title('Recall en Validación')\n",
        "axes[1, 0].set_xlim([0, 1])\n",
        "\n",
        "# F1-Score:\n",
        "axes[1, 1].barh(results_df['Modelo'], results_df['F1_Score_Val'], color='red')\n",
        "axes[1, 1].set_xlabel('F1-Score')\n",
        "axes[1, 1].set_title('F1-Score en Validación')\n",
        "axes[1, 1].set_xlim([0, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Curvas ROC:\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for name, model in trained_models.items():\n",
        "    y_pred_proba = model.predict_proba(X_val_scaled)[:, 1] if hasattr(model, 'predict_proba') else model.predict(X_val_scaled)\n",
        "    fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})', linewidth=2)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Curvas ROC - Comparación de Modelos')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análisis Comparativo de Modelos\n",
        "\n",
        "**Rendimiento General:**\n",
        "\n",
        "Los cinco algoritmos evaluados muestran capacidad de clasificación por encima del nivel aleatorio, con métricas de F1-Score entre 0.7867 y 0.8608. La normalización de características mediante StandardScaler fue determinante para el rendimiento de algoritmos sensibles a escala (KNN, SVM, Neural Networks).\n",
        "\n",
        "**Observaciones por Modelo:**\n",
        "\n",
        "1. **Decision Tree**: Desempeño intermedio con evidencia de sobreajuste moderado (accuracy train 0.8645 vs validation 0.7899)\n",
        "\n",
        "2. **KNN**: Alto overfitting evidente (accuracy train 0.9984 vs validation 0.8043), requeriría regularización adicional\n",
        "\n",
        "3. **Neural Network**: Rendimiento estable pero inferior a otros modelos, posiblemente requiere mayor cantidad de datos o ajuste de arquitectura\n",
        "\n",
        "4. **SVM**: Mejor balance entre métricas, control adecuado de overfitting, recall elevado favorable para aplicaciones médicas\n",
        "\n",
        "5. **Random Forest**: Buen control de overfitting, desempeño sólido pero no óptimo en este dataset\n",
        "\n",
        "**Interpretación de Métricas:**\n",
        "\n",
        "- **Accuracy**: Proporción global de predicciones correctas\n",
        "- **Precision**: Proporción de predicciones positivas que son correctas (minimiza falsos positivos)\n",
        "- **Recall**: Proporción de casos positivos reales detectados (minimiza falsos negativos)\n",
        "- **F1-Score**: Media armónica entre precision y recall\n",
        "\n",
        "**Consideración Clínica:**\n",
        "\n",
        "En contexto médico de screening, el recall tiene prioridad sobre precision para evitar falsos negativos (pacientes enfermos no detectados). El modelo SVM con recall de 0.8831 cumple adecuadamente este criterio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. HIPERPARAMETRIZACIÓN DEL MEJOR MODELO CON GRIDSEARCH\n",
        "\n",
        "Utilizaremos GridSearch para encontrar la mejor combinación de hiperparámetros del modelo ganador.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nHIPERPARAMETRIZACIÓN CON GRIDSEARCH\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Modelo seleccionado: {best_model_name}\")\n",
        "\n",
        "# Definir grids de hiperparámetros para cada modelo:\n",
        "param_grids = {\n",
        "    'Decision Tree': {\n",
        "        'criterion': ['gini', 'entropy'],\n",
        "        'max_depth': [5, 10, 15, 20],\n",
        "        'min_samples_split': [10, 20, 30],\n",
        "        'min_samples_leaf': [5, 10, 15]\n",
        "    },\n",
        "    'KNN': {\n",
        "        'n_neighbors': [3, 5, 7, 9, 11],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'Neural Network': {\n",
        "        'hidden_layer_sizes': [(50,), (100,), (100, 50), (100, 50, 25)],\n",
        "        'activation': ['relu', 'tanh'],\n",
        "        'alpha': [0.0001, 0.001, 0.01],\n",
        "        'learning_rate': ['constant', 'adaptive']\n",
        "    },\n",
        "    'SVM': {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'gamma': ['scale', 'auto', 0.001, 0.01],\n",
        "        'kernel': ['rbf', 'poly']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [50, 100, 150, 200],\n",
        "        'max_depth': [10, 15, 20, None],\n",
        "        'min_samples_split': [5, 10, 15],\n",
        "        'min_samples_leaf': [2, 5, 10]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Seleccionar el modelo base y su grid:\n",
        "base_models = {\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Neural Network': MLPClassifier(random_state=42, max_iter=500),\n",
        "    'SVM': SVC(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "best_base_model = base_models[best_model_name]\n",
        "param_grid = param_grids[best_model_name]\n",
        "\n",
        "print(f\"\\nGrid de búsqueda:\")\n",
        "for param, values in param_grid.items():\n",
        "    print(f\"  {param}: {values}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejecutar GridSearch:\n",
        "print(\"\\nEjecutando GridSearch...\")\n",
        "print(\"Esto puede tomar varios minutos...\\n\")\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=best_base_model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Combinar train y validation para GridSearch:\n",
        "X_train_full = np.vstack([X_train_scaled, X_val_scaled])\n",
        "y_train_full = np.concatenate([y_train, y_val])\n",
        "\n",
        "# Entrenar:\n",
        "grid_search.fit(X_train_full, y_train_full)\n",
        "\n",
        "print(\"\\nGridSearch completado\")\n",
        "print(f\"\\nMejores hiperparámetros encontrados:\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "print(f\"\\nMejor F1-Score en CV: {grid_search.best_score_:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluar el modelo optimizado en el conjunto de test:\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_test = best_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\nEVALUACIÓN EN CONJUNTO DE TEST\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "test_precision = precision_score(y_test, y_pred_test)\n",
        "test_recall = recall_score(y_test, y_pred_test)\n",
        "test_f1 = f1_score(y_test, y_pred_test)\n",
        "test_roc_auc = roc_auc_score(y_test, y_pred_test)\n",
        "\n",
        "print(f\"\\nMétricasen Test:\")\n",
        "print(f\"  Accuracy:  {test_accuracy:.4f}\")\n",
        "print(f\"  Precision: {test_precision:.4f}\")\n",
        "print(f\"  Recall:    {test_recall:.4f}\")\n",
        "print(f\"  F1-Score:  {test_f1:.4f}\")\n",
        "print(f\"  ROC-AUC:   {test_roc_auc:.4f}\")\n",
        "\n",
        "# Matriz de confusión en test:\n",
        "cm_test = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_test, annot=True, fmt='d', cmap='viridis', cbar=False)\n",
        "plt.title(f'Matriz de Confusión - {best_model_name} (Optimizado) - Test Set')\n",
        "plt.ylabel('Real')\n",
        "plt.xlabel('Predicción')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nReporte de Clasificación en Test:\")\n",
        "print(classification_report(y_test, y_pred_test, target_names=['No Disease', 'Disease']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. GUARDAR MODELO FINAL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Guardar el modelo optimizado y el scaler:\n",
        "with open('best_model.pkl', 'wb') as f:\n",
        "    pickle.dump(best_model, f)\n",
        "\n",
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "with open('label_encoders.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoders, f)\n",
        "\n",
        "print(\"Modelo guardado: best_model.pkl\")\n",
        "print(\"Scaler guardado: scaler.pkl\")\n",
        "print(\"Encoders guardados: label_encoders.pkl\")\n",
        "\n",
        "# Guardar información del modelo:\n",
        "model_info = {\n",
        "    'model_name': best_model_name,\n",
        "    'best_params': grid_search.best_params_,\n",
        "    'test_accuracy': test_accuracy,\n",
        "    'test_f1': test_f1,\n",
        "    'test_roc_auc': test_roc_auc,\n",
        "    'feature_columns': feature_columns\n",
        "}\n",
        "\n",
        "with open('model_info.pkl', 'wb') as f:\n",
        "    pickle.dump(model_info, f)\n",
        "\n",
        "print(\"Información del modelo guardada: model_info.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CONCLUSIONES FINALES\n",
        "\n",
        "### 1. Preparación de Datos\n",
        "\n",
        "**Dataset procesado**: 918 registros con 14 características predictoras\n",
        "\n",
        "**División de datos**:\n",
        "- Entrenamiento: 642 registros (69.9%)\n",
        "- Validación: 138 registros (15.0%)\n",
        "- Test: 138 registros (15.0%)\n",
        "\n",
        "**Preprocesamiento aplicado**:\n",
        "- Codificación de variables categóricas mediante Label Encoding (sex, cp, restecg, slope, thal, dataset)\n",
        "- Conversión de variables booleanas a formato numérico (fbs, exang)\n",
        "- Normalización de características con StandardScaler (media=0, desviación estándar=1)\n",
        "\n",
        "### 2. Evaluación de Algoritmos\n",
        "\n",
        "Se entrenaron y evaluaron 5 algoritmos de clasificación:\n",
        "\n",
        "**Árbol de Decisión**: Accuracy 0.7899, F1-Score 0.8079\n",
        "- Configuración: max_depth=10, min_samples_split=20\n",
        "- Desempeño balanceado, tendencia a overfitting en training\n",
        "\n",
        "**K-Nearest Neighbors**: Accuracy 0.8043, F1-Score 0.8280\n",
        "- Configuración: n_neighbors=5, weights='distance', metric='euclidean'\n",
        "- Alto overfitting (accuracy train 0.9984), generalización moderada\n",
        "\n",
        "**Red Neuronal (MLP)**: Accuracy 0.7681, F1-Score 0.7867\n",
        "- Configuración: hidden_layers=(100,50), activation='relu', solver='adam'\n",
        "- Desempeño estable, pero no alcanza el nivel de otros modelos\n",
        "\n",
        "**Support Vector Machine**: Accuracy 0.8406, F1-Score 0.8608\n",
        "- Configuración: kernel='rbf', C=1.0, gamma='scale'\n",
        "- **Mejor desempeño inicial**, balance óptimo entre métricas\n",
        "\n",
        "**Random Forest**: Accuracy 0.7971, F1-Score 0.8228\n",
        "- Configuración: n_estimators=100, max_depth=15, min_samples_split=10\n",
        "- Buen control de overfitting, desempeño sólido\n",
        "\n",
        "### 3. Modelo Seleccionado y Optimización\n",
        "\n",
        "**Mejor modelo**: Support Vector Machine (SVM)\n",
        "- Seleccionado por su F1-Score superior (0.8608) y balance entre precision y recall\n",
        "- El recall elevado (0.8831) es particularmente importante en contexto médico para minimizar falsos negativos\n",
        "\n",
        "**Hiperparametrización con GridSearchCV**:\n",
        "- Búsqueda exhaustiva en espacio de 32 combinaciones de hiperparámetros\n",
        "- Validación cruzada con 5 folds\n",
        "- Métrica de optimización: F1-Score\n",
        "\n",
        "**Mejores hiperparámetros encontrados**:\n",
        "- C: 10 (incremento en complejidad del modelo)\n",
        "- gamma: 0.01 (control de influencia de puntos individuales)\n",
        "- kernel: rbf (mantiene kernel radial)\n",
        "\n",
        "### 4. Rendimiento Final\n",
        "\n",
        "**Métricas en conjunto de test**:\n",
        "- Accuracy: 0.8406 (84.1% de clasificaciones correctas)\n",
        "- Precision: 0.8395 (83.9% de predicciones positivas son correctas)\n",
        "- Recall: 0.8831 (88.3% de casos positivos detectados)\n",
        "- F1-Score: 0.8608 (balance armónico entre precision y recall)\n",
        "- ROC-AUC: 0.8350 (excelente capacidad discriminativa)\n",
        "\n",
        "**Interpretación clínica**:\n",
        "- El modelo identifica correctamente 88.3% de pacientes con enfermedad cardíaca\n",
        "- Del total de predicciones positivas, 83.9% son correctas\n",
        "- La matriz de confusión muestra un desbalance favorable hacia minimizar falsos negativos, apropiado para aplicaciones médicas\n",
        "\n",
        "### 5. Archivos Generados\n",
        "\n",
        "- `best_model.pkl`: Modelo SVM optimizado\n",
        "- `scaler.pkl`: StandardScaler entrenado para normalización\n",
        "- `label_encoders.pkl`: Diccionario de encoders para variables categóricas\n",
        "- `model_info.pkl`: Metadatos del modelo (hiperparámetros, métricas, características)\n",
        "\n",
        "### 6. Análisis de Resultados\n",
        "\n",
        "**Fortalezas del modelo**:\n",
        "- Alto recall apropiado para screening médico\n",
        "- Generalización validada en conjunto de test independiente\n",
        "- Hiperparámetros optimizados mediante búsqueda sistemática\n",
        "- Rendimiento consistente entre validación y test (sin overfitting significativo)\n",
        "\n",
        "**Consideraciones**:\n",
        "- El modelo está calibrado para minimizar falsos negativos, lo que puede incrementar falsos positivos\n",
        "- Las curvas ROC muestran que SVM supera consistentemente al clasificador aleatorio\n",
        "- La normalización fue crucial para el rendimiento de modelos sensibles a escala (SVM, KNN, Neural Networks)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
