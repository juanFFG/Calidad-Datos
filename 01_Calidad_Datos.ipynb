{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALIDAD DE DATOS - Heart Disease UCI Dataset\n",
    "\n",
    "## Contenido:\n",
    "1. Perfilado de Datos (Profiling)\n",
    "2. Diagnóstico de Dimensiones de Calidad\n",
    "3. Limpieza y Mejora de Datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PERFILADO DE DATOS\n",
    "\n",
    "Generaremos un reporte completo utilizando ydata-profiling para entender la estructura y características de los datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dependencias instaladas correctamente\n"
     ]
    }
   ],
   "source": [
    "# Instalar dependencias:\n",
    "%pip install pandas numpy matplotlib seaborn ydata-profiling scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ydata_profiling import ProfileReport\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (920, 16)\n",
      "Número de registros: 920\n",
      "Número de variables: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>dataset</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalch</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>typical angina</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>True</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>150.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.3</td>\n",
       "      <td>downsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fixed defect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>108.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.5</td>\n",
       "      <td>flat</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>129.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.6</td>\n",
       "      <td>flat</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable defect</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>Male</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>non-anginal</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>False</td>\n",
       "      <td>normal</td>\n",
       "      <td>187.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.5</td>\n",
       "      <td>downsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>Female</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>atypical angina</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>False</td>\n",
       "      <td>lv hypertrophy</td>\n",
       "      <td>172.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.4</td>\n",
       "      <td>upsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  age     sex    dataset               cp  trestbps   chol    fbs  \\\n",
       "0   1   63    Male  Cleveland   typical angina     145.0  233.0   True   \n",
       "1   2   67    Male  Cleveland     asymptomatic     160.0  286.0  False   \n",
       "2   3   67    Male  Cleveland     asymptomatic     120.0  229.0  False   \n",
       "3   4   37    Male  Cleveland      non-anginal     130.0  250.0  False   \n",
       "4   5   41  Female  Cleveland  atypical angina     130.0  204.0  False   \n",
       "\n",
       "          restecg  thalch  exang  oldpeak        slope   ca  \\\n",
       "0  lv hypertrophy   150.0  False      2.3  downsloping  0.0   \n",
       "1  lv hypertrophy   108.0   True      1.5         flat  3.0   \n",
       "2  lv hypertrophy   129.0   True      2.6         flat  2.0   \n",
       "3          normal   187.0  False      3.5  downsloping  0.0   \n",
       "4  lv hypertrophy   172.0  False      1.4    upsloping  0.0   \n",
       "\n",
       "                thal  num  \n",
       "0       fixed defect    0  \n",
       "1             normal    2  \n",
       "2  reversable defect    1  \n",
       "3             normal    0  \n",
       "4             normal    0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar datos:\n",
    "df_original = pd.read_csv('heart_disease_uci.csv')\n",
    "print(f\"Dimensiones del dataset: {df_original.shape}\")\n",
    "print(f\"Número de registros: {df_original.shape[0]}\")\n",
    "print(f\"Número de variables: {df_original.shape[1]}\")\n",
    "df_original.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversión Inicial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convirtiendo variables categóricas...\n",
      "✓ sex: category\n",
      "✓ dataset: category\n",
      "✓ cp: category\n",
      "✓ fbs: category\n",
      "✓ restecg: category\n",
      "✓ exang: category\n",
      "✓ slope: category\n",
      "✓ thal: category\n",
      "\n",
      "=== TIPOS DE DATOS ACTUALIZADOS ===\n",
      "id             int64\n",
      "age            int64\n",
      "sex         category\n",
      "dataset     category\n",
      "cp          category\n",
      "trestbps     float64\n",
      "chol         float64\n",
      "fbs         category\n",
      "restecg     category\n",
      "thalch       float64\n",
      "exang       category\n",
      "oldpeak      float64\n",
      "slope       category\n",
      "ca           float64\n",
      "thal        category\n",
      "num            int64\n",
      "dtype: object\n",
      "\n",
      "Uso de memoria optimizado: 66.84 KB\n"
     ]
    }
   ],
   "source": [
    "# Convertir variables categóricas:\n",
    "categorical_vars = ['sex', 'dataset', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "\n",
    "for col in categorical_vars:\n",
    "    if col in df_original.columns:\n",
    "        df_original[col] = df_original[col].astype('category')\n",
    "        print(f\"{col}: {df_original[col].dtype}\")\n",
    "\n",
    "print(\"\\nTIPOS DE DATOS ACTUALIZADOS\")\n",
    "print(df_original.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 920 entries, 0 to 919\n",
      "Data columns (total 16 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   id        920 non-null    int64   \n",
      " 1   age       920 non-null    int64   \n",
      " 2   sex       920 non-null    category\n",
      " 3   dataset   920 non-null    category\n",
      " 4   cp        920 non-null    category\n",
      " 5   trestbps  861 non-null    float64 \n",
      " 6   chol      890 non-null    float64 \n",
      " 7   fbs       830 non-null    category\n",
      " 8   restecg   918 non-null    category\n",
      " 9   thalch    865 non-null    float64 \n",
      " 10  exang     865 non-null    category\n",
      " 11  oldpeak   858 non-null    float64 \n",
      " 12  slope     611 non-null    category\n",
      " 13  ca        309 non-null    float64 \n",
      " 14  thal      434 non-null    category\n",
      " 15  num       920 non-null    int64   \n",
      "dtypes: category(8), float64(5), int64(3)\n",
      "memory usage: 65.9 KB\n"
     ]
    }
   ],
   "source": [
    "df_original.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando reporte de perfilado de datos...\n",
      "Esto puede tardar 2-3 minutos, por favor espera...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fa1045e6a5421aad50ab5c2af9087b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generar reporte de perfilado;:\n",
    "profile = ProfileReport(df_original, \n",
    "                       title=\"Heart Disease UCI - Reporte de Perfilado\",\n",
    "                       minimal=True)\n",
    "\n",
    "# Guardar reporte:\n",
    "profile.to_file(\"heart_disease_profiling_report.html\")\n",
    "print(\"heart_disease_profiling_report.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DIAGNÓSTICO DE CALIDAD DE DATOS\n",
    "\n",
    "Analizaremos las siguientes dimensiones de calidad:\n",
    "- **Completitud**: Presencia de valores faltantes\n",
    "- **Consistencia**: Coherencia de los valores\n",
    "- **Exactitud**: Valores fuera de rango o anómalos\n",
    "- **Unicidad**: Duplicados\n",
    "- **Validez**: Tipos de datos correctos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 COMPLETITUD (Valores Faltantes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar cadenas vacías por NaN:\n",
    "df_analysis = df_original.replace('', np.nan)\n",
    "\n",
    "# Análisis de valores faltantes:\n",
    "missing_data = pd.DataFrame({\n",
    "    'Columna': df_analysis.columns,\n",
    "    'Valores_Faltantes': df_analysis.isnull().sum(),\n",
    "    'Porcentaje': (df_analysis.isnull().sum() / len(df_analysis) * 100).round(2)\n",
    "}).sort_values('Valores_Faltantes', ascending=False)\n",
    "\n",
    "print(\"\\nANÁLISIS DE COMPLETITUD\")\n",
    "print(missing_data[missing_data['Valores_Faltantes'] > 0])\n",
    "\n",
    "# Visualización:\n",
    "plt.figure(figsize=(12, 6))\n",
    "missing_cols = missing_data[missing_data['Valores_Faltantes'] > 0]\n",
    "if len(missing_cols) > 0:\n",
    "    plt.barh(missing_cols['Columna'], missing_cols['Porcentaje'])\n",
    "    plt.xlabel('Porcentaje de Valores Faltantes (%)')\n",
    "    plt.title('Completitud de Datos por Variable')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay valores faltantes en el dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diagnóstico de Completitud**\n",
    "\n",
    "Se identificaron valores faltantes en 10 variables del dataset:\n",
    "\n",
    "- Variables con alto porcentaje de valores faltantes (>30%):\n",
    "  - ca (66.41%): número de vasos principales coloreados por fluoroscopia\n",
    "  - thal (52.83%): resultados de pruebas de talasemia\n",
    "  - slope (33.59%): pendiente del segmento ST del ejercicio\n",
    "\n",
    "- Variables numéricas con valores faltantes moderados:\n",
    "  - trestbps (6.41%), chol (3.26%), thalch (5.98%), oldpeak (6.74%)\n",
    "\n",
    "- Variables categóricas con valores faltantes:\n",
    "  - fbs (9.78%), restecg (0.22%), exang (5.98%)\n",
    "\n",
    "Los valores faltantes se deben probablemente a la consolidación de datos de diferentes centros médicos (Cleveland, Hungary, Switzerland, VA Long Beach) con diferentes protocolos de recolección de información.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 CONSISTENCIA (Valores Inconsistentes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nANÁLISIS DE CONSISTENCIA\")\n",
    "\n",
    "# Verificar consistencia de valores categóricos:\n",
    "categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_original.columns:\n",
    "        print(f\"\\n{col.upper()}:\")\n",
    "        print(df_original[col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar rangos de variables numéricas:\n",
    "print(\"\\nRANGOS DE VARIABLES NUMÉRICAS\")\n",
    "numeric_cols = ['age', 'trestbps', 'chol', 'thalch', 'oldpeak', 'ca']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df_analysis.columns:\n",
    "        # Convertir a numérico:z\n",
    "        df_analysis[col] = pd.to_numeric(df_analysis[col], errors='coerce')\n",
    "        print(f\"\\n{col.upper()}:\")\n",
    "        print(f\"  Min: {df_analysis[col].min()}\")\n",
    "        print(f\"  Max: {df_analysis[col].max()}\")\n",
    "        print(f\"  Media: {df_analysis[col].mean():.2f}\")\n",
    "        print(f\"  Desv. Std: {df_analysis[col].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diagnóstico de Consistencia**\n",
    "\n",
    "Se identificaron las siguientes inconsistencias en los datos:\n",
    "\n",
    "1. Formato de variables booleanas: Las variables `fbs` y `exang` contienen valores \"TRUE\"/\"FALSE\" como texto en lugar de tipos booleanos nativos.\n",
    "\n",
    "2. Valores anómalos: Se detectaron valores de 0 en variables donde no son clínicamente válidos:\n",
    "   - trestbps (presión arterial en reposo): 1 registro con valor 0\n",
    "   - chol (colesterol sérico): 172 registros con valor 0\n",
    "\n",
    "3. Distribución de variables categóricas: La distribución es consistente con estudios epidemiológicos (mayor prevalencia de pacientes masculinos: 79%, tipos de dolor torácico con mayoría asintomática: 54%).\n",
    "\n",
    "Estas inconsistencias son típicas de la consolidación de datos clínicos de múltiples centros médicos sin un proceso previo de estandarización.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 EXACTITUD (Valores Fuera de Rango)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nANÁLISIS DE EXACTITUD\")\n",
    "\n",
    "# Definir rangos:\n",
    "ranges = {\n",
    "    'age': (0, 120),\n",
    "    'trestbps': (80, 220),  # Presión arterial en reposo\n",
    "    'chol': (100, 600),     # Colesterol sérico\n",
    "    'thalch': (60, 220),    # Ritmo cardíaco máximo\n",
    "    'oldpeak': (0, 10),     # Depresión ST\n",
    "    'ca': (0, 4)            # Número de vasos principales\n",
    "}\n",
    "\n",
    "outliers_summary = []\n",
    "\n",
    "for col, (min_val, max_val) in ranges.items():\n",
    "    if col in df_analysis.columns:\n",
    "        out_of_range = df_analysis[\n",
    "            (df_analysis[col] < min_val) | (df_analysis[col] > max_val)\n",
    "        ][col].dropna()\n",
    "        \n",
    "        if len(out_of_range) > 0:\n",
    "            outliers_summary.append({\n",
    "                'Variable': col,\n",
    "                'Rango_Esperado': f\"{min_val}-{max_val}\",\n",
    "                'Valores_Fuera_Rango': len(out_of_range),\n",
    "                'Porcentaje': f\"{(len(out_of_range)/len(df_analysis)*100):.2f}%\"\n",
    "            })\n",
    "\n",
    "if outliers_summary:\n",
    "    outliers_df = pd.DataFrame(outliers_summary)\n",
    "    print(outliers_df)\n",
    "else:\n",
    "    print(\"No se encontraron valores fuera de rango\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar outliers usando IQR\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return len(outliers), lower_bound, upper_bound\n",
    "\n",
    "print(\"\\nDETECCIÓN DE OUTLIERS (Método IQR)\")\n",
    "for col in numeric_cols:\n",
    "    if col in df_analysis.columns:\n",
    "        n_outliers, lower, upper = detect_outliers_iqr(df_analysis, col)\n",
    "        if n_outliers > 0:\n",
    "            print(f\"\\n{col.upper()}: {n_outliers} outliers detectados\")\n",
    "            print(f\"  Límite inferior: {lower:.2f}\")\n",
    "            print(f\"  Límite superior: {upper:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diagnóstico de Exactitud**\n",
    "\n",
    "Análisis de valores fuera de rangos clínicos esperados:\n",
    "\n",
    "1. Valores fuera de rango por rangos médicos establecidos:\n",
    "   - trestbps: 1 valor fuera del rango 80-220 mmHg (0.11%)\n",
    "   - chol: 174 valores fuera del rango 100-600 mg/dl (18.91%)\n",
    "   - oldpeak: 12 valores fuera del rango 0-10 (1.30%)\n",
    "\n",
    "2. Outliers detectados mediante método IQR:\n",
    "   - trestbps: 28 outliers (valores extremos de presión arterial)\n",
    "   - chol: 183 outliers (principalmente valores de 0 y valores muy altos)\n",
    "   - thalch: 2 outliers (frecuencia cardíaca máxima atípica)\n",
    "   - oldpeak: 16 outliers (depresión ST extrema)\n",
    "   - ca: 20 outliers (valores en el límite superior)\n",
    "\n",
    "Los valores de 0 en chol y trestbps son clínicamente imposibles y representan probablemente marcadores de datos faltantes que no fueron codificados correctamente en la fuente original.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 UNICIDAD (Duplicados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nANÁLISIS DE UNICIDAD\")\n",
    "\n",
    "# Verificar duplicados basados en ID\n",
    "duplicate_ids = df_original['id'].duplicated().sum()\n",
    "print(f\"Registros con ID duplicado: {duplicate_ids}\")\n",
    "\n",
    "# Verificar duplicados exactos en todas las columnas (excepto ID)\n",
    "cols_without_id = [col for col in df_original.columns if col != 'id']\n",
    "duplicate_rows = df_original.duplicated(subset=cols_without_id, keep=False).sum()\n",
    "print(f\"Registros completamente duplicados: {duplicate_rows}\")\n",
    "\n",
    "if duplicate_rows > 0:\n",
    "    print(\"\\nEjemplos de registros duplicados:\")\n",
    "    print(df_original[df_original.duplicated(subset=cols_without_id, keep=False)].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diagnóstico de Unicidad**\n",
    "\n",
    "Resultados del análisis de duplicación:\n",
    "\n",
    "- IDs únicos: No se encontraron IDs duplicados (920 IDs únicos)\n",
    "- Registros duplicados: 4 registros completamente duplicados (0.43% del dataset)\n",
    "\n",
    "Los registros duplicados identificados corresponden a 2 pares de pacientes con información idéntica en todas las variables clínicas. Estos duplicados se originan probablemente durante el proceso de consolidación de los datasets de diferentes centros médicos (Hungary y VA Long Beach en los ejemplos identificados).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 VALIDEZ (Tipos de Datos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nANÁLISIS DE VALIDEZ\")\n",
    "print(\"\\nTipos de datos actuales:\")\n",
    "print(df_original.dtypes)\n",
    "\n",
    "print(\"\\n\\nTipos de datos esperados:\")\n",
    "expected_types = {\n",
    "    'id': 'int',\n",
    "    'age': 'int',\n",
    "    'sex': 'categorical',\n",
    "    'dataset': 'categorical',\n",
    "    'cp': 'categorical',\n",
    "    'trestbps': 'float',\n",
    "    'chol': 'float',\n",
    "    'fbs': 'boolean',\n",
    "    'restecg': 'categorical',\n",
    "    'thalch': 'float',\n",
    "    'exang': 'boolean',\n",
    "    'oldpeak': 'float',\n",
    "    'slope': 'categorical',\n",
    "    'ca': 'float',\n",
    "    'thal': 'categorical',\n",
    "    'num': 'int (target)'\n",
    "}\n",
    "\n",
    "for col, expected_type in expected_types.items():\n",
    "    print(f\"{col}: {expected_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diagnóstico de Validez**\n",
    "\n",
    "Evaluación de tipos de datos:\n",
    "\n",
    "Tipos de datos correctos:\n",
    "- Variables identificadoras y numéricas: `id`, `age`, `num` (int64)\n",
    "- Variables continuas: `trestbps`, `chol`, `thalch`, `oldpeak`, `ca` (float64)\n",
    "- Variables categóricas: `sex`, `dataset`, `cp`, `restecg`, `slope`, `thal` (category)\n",
    "\n",
    "Tipos de datos que requieren conversión:\n",
    "- `fbs` y `exang`: Almacenadas como category con valores de texto \"TRUE\"/\"FALSE\" en lugar de tipos booleanos\n",
    "\n",
    "El dataset presenta una estructura de tipos de datos generalmente apropiada después de la conversión inicial a tipos categóricos. Solo se requiere conversión explícita de las variables booleanas para optimizar el procesamiento y modelado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LIMPIEZA Y MEJORA DE DATOS\n",
    "\n",
    "Implementaremos las transformaciones necesarias para resolver los problemas identificados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Preparación Inicial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear copia para limpieza:\n",
    "df_clean = df_original.copy()\n",
    "\n",
    "print(\"Dataset antes de limpieza:\")\n",
    "print(f\"Shape: {df_clean.shape}\")\n",
    "print(f\"Valores faltantes totales: {df_clean.replace('', np.nan).isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Conversión de Tipos de Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPASO 1: CONVERSIÓN DE TIPOS DE DATOS\")\n",
    "\n",
    "# Reemplazar strings vacíos por NaN:\n",
    "df_clean = df_clean.replace('', np.nan)\n",
    "\n",
    "# Convertir variables booleanas:\n",
    "bool_cols = ['fbs', 'exang']\n",
    "for col in bool_cols:\n",
    "    df_clean[col] = df_clean[col].map({'TRUE': True, 'FALSE': False, True: True, False: False})\n",
    "    print(f\"{col} convertido a booleano\")\n",
    "\n",
    "# Convertir variables numéricas:\n",
    "numeric_cols_convert = ['trestbps', 'chol', 'thalch', 'oldpeak', 'ca']\n",
    "for col in numeric_cols_convert:\n",
    "    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "    print(f\"{col} convertido a numérico\")\n",
    "\n",
    "print(\"\\nTipos de datos después de conversión:\")\n",
    "print(df_clean.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Manejo de Valores Anómalos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPASO 2: MANEJO DE VALORES ANÓMALOS\")\n",
    "\n",
    "# Reemplazar valores de 0 por NaN en variables donde 0 no es válido:\n",
    "zero_invalid_cols = ['trestbps', 'chol', 'thalch']\n",
    "for col in zero_invalid_cols:\n",
    "    n_zeros = (df_clean[col] == 0).sum()\n",
    "    if n_zeros > 0:\n",
    "        df_clean.loc[df_clean[col] == 0, col] = np.nan\n",
    "        print(f\"{col}: {n_zeros} valores de 0 convertidos a NaN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Imputación de Valores Faltantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPASO 3: IMPUTACIÓN DE VALORES FALTANTES\")\n",
    "\n",
    "# Contar valores faltantes antes de imputación:\n",
    "missing_before = df_clean.isnull().sum()\n",
    "print(\"\\nValores faltantes antes de imputación:\")\n",
    "print(missing_before[missing_before > 0])\n",
    "\n",
    "# Estrategia de imputación:\n",
    "# - Variables numéricas: imputar con mediana por dataset origen (fallback: mediana global)\n",
    "# - Variables categóricas: imputar con moda por dataset origen (fallback: moda global)\n",
    "\n",
    "# Imputación de variables numéricas por dataset:\n",
    "numeric_to_impute = ['trestbps', 'chol', 'thalch', 'oldpeak', 'ca']\n",
    "\n",
    "for col in numeric_to_impute:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        # Calcular mediana global como fallback:\n",
    "        global_median = df_clean[col].median()\n",
    "        \n",
    "        # Imputar con la mediana del dataset correspondiente:\n",
    "        for dataset in df_clean['dataset'].unique():\n",
    "            mask = (df_clean['dataset'] == dataset) & (df_clean[col].isnull())\n",
    "            if mask.sum() > 0:\n",
    "                median_val = df_clean[df_clean['dataset'] == dataset][col].median()\n",
    "                # Si el dataset no tiene valores válidos, usar mediana global:\n",
    "                if pd.isna(median_val):\n",
    "                    median_val = global_median\n",
    "                df_clean.loc[mask, col] = median_val\n",
    "        print(f\"{col} imputado con mediana por dataset\")\n",
    "\n",
    "# Imputación de variables categóricas (booleanas y nominales):\n",
    "categorical_to_impute = ['fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "\n",
    "for col in categorical_to_impute:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        original_dtype = df_clean[col].dtype\n",
    "        df_clean[col] = df_clean[col].astype('object')\n",
    "        \n",
    "        # Calcular moda global como fallback:\n",
    "        global_mode = df_clean[col].mode()\n",
    "        global_mode_val = global_mode.iloc[0] if len(global_mode) > 0 else None\n",
    "        \n",
    "        # Imputar con la moda del dataset correspondiente:\n",
    "        for dataset in df_clean['dataset'].unique():\n",
    "            mask = (df_clean['dataset'] == dataset) & (df_clean[col].isnull())\n",
    "            if mask.sum() > 0:\n",
    "                mode_val = df_clean[df_clean['dataset'] == dataset][col].mode()\n",
    "                if len(mode_val) > 0:\n",
    "                    fill_value = mode_val.iloc[0]\n",
    "                else:\n",
    "                    # Si el dataset no tiene valores válidos, usar moda global:\n",
    "                    fill_value = global_mode_val\n",
    "                \n",
    "                if fill_value is not None:\n",
    "                    df_clean.loc[mask, col] = fill_value\n",
    "        \n",
    "        df_clean[col] = df_clean[col].astype('category')\n",
    "        print(f\"{col} imputado con moda por dataset\")\n",
    "\n",
    "# Valores faltantes después de imputación\n",
    "missing_after = df_clean.isnull().sum()\n",
    "print(\"\\nValores faltantes después de imputación:\")\n",
    "if missing_after.sum() == 0:\n",
    "    print(\"No quedan valores faltantes en el dataset\")\n",
    "else:\n",
    "    print(missing_after[missing_after > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Eliminación de Duplicados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPASO 4: ELIMINACIÓN DE DUPLICADOS\")\n",
    "\n",
    "# Eliminar filas completamente duplicadas:\n",
    "cols_for_dup = [col for col in df_clean.columns if col != 'id']\n",
    "duplicates = df_clean.duplicated(subset=cols_for_dup).sum()\n",
    "\n",
    "if duplicates > 0:\n",
    "    df_clean = df_clean.drop_duplicates(subset=cols_for_dup, keep='first')\n",
    "    print(f\"Eliminados {duplicates} registros duplicados\")\n",
    "else:\n",
    "    print(\"No se encontraron registros duplicados\")\n",
    "\n",
    "print(f\"\\nShape después de eliminar duplicados: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Creación de Variable Target Binaria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPASO 5: CREACIÓN DE VARIABLE TARGET\")\n",
    "\n",
    "# Crear variable binaria: 0 = sin enfermedad, 1 = con enfermedad (cualquier grado):\n",
    "df_clean['heart_disease'] = (df_clean['num'] > 0).astype(int)\n",
    "\n",
    "print(\"\\nDistribución de la variable target:\")\n",
    "print(df_clean['heart_disease'].value_counts())\n",
    "print(\"\\nPorcentajes:\")\n",
    "print(df_clean['heart_disease'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualización:\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "df_clean['num'].value_counts().sort_index().plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Distribución Original (num: 0-4)')\n",
    "axes[0].set_xlabel('Nivel de Enfermedad')\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "\n",
    "df_clean['heart_disease'].value_counts().plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Variable Target Binaria')\n",
    "axes[1].set_xlabel('Heart Disease (0=No, 1=Yes)')\n",
    "axes[1].set_ylabel('Frecuencia')\n",
    "axes[1].set_xticklabels(['No', 'Yes'], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Validación Final de Limpieza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRESUMEN DE LIMPIEZA\")\n",
    "print(f\"\\nRegistros originales: {df_original.shape[0]}\")\n",
    "print(f\"Registros después de limpieza: {df_clean.shape[0]}\")\n",
    "print(f\"Registros eliminados: {df_original.shape[0] - df_clean.shape[0]}\")\n",
    "\n",
    "print(\"\\nCALIDAD FINAL DE DATOS\")\n",
    "missing_final = df_clean.isnull().sum()\n",
    "print(f\"\\nValores faltantes totales: {missing_final.sum()}\")\n",
    "print(f\"Columnas con valores faltantes: {(missing_final > 0).sum()}\")\n",
    "\n",
    "if missing_final.sum() > 0:\n",
    "    print(\"\\nColumnas con valores faltantes restantes:\")\n",
    "    print(missing_final[missing_final > 0])\n",
    "\n",
    "print(\"\\nTipos de datos finales:\")\n",
    "print(df_clean.dtypes)\n",
    "\n",
    "print(\"\\nESTADÍSTICAS DESCRIPTIVAS FINALES\")\n",
    "print(df_clean.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Visualización de Mejoras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación antes y después:\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Valores faltantes antes:\n",
    "df_original_analysis = df_original.replace('', np.nan)\n",
    "missing_original = df_original_analysis.isnull().sum().sort_values(ascending=False)[:10]\n",
    "\n",
    "axes[0].barh(range(len(missing_original)), missing_original.values)\n",
    "axes[0].set_yticks(range(len(missing_original)))\n",
    "axes[0].set_yticklabels(missing_original.index)\n",
    "axes[0].set_xlabel('Número de Valores Faltantes')\n",
    "axes[0].set_title('ANTES: Valores Faltantes')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Valores faltantes después:\n",
    "missing_clean = df_clean.isnull().sum().sort_values(ascending=False)[:10]\n",
    "\n",
    "axes[1].barh(range(len(missing_clean)), missing_clean.values, color='green')\n",
    "axes[1].set_yticks(range(len(missing_clean)))\n",
    "axes[1].set_yticklabels(missing_clean.index)\n",
    "axes[1].set_xlabel('Número de Valores Faltantes')\n",
    "axes[1].set_title('DESPUÉS: Valores Faltantes')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Exportar Datos Limpios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar dataset limpio:\n",
    "df_clean.to_csv('heart_disease_clean.csv', index=False)\n",
    "print(\"Dataset limpio guardado como: heart_disease_clean.csv\")\n",
    "\n",
    "# Mostrar primeras filas del dataset limpio:\n",
    "print(\"\\nPrimeras filas del dataset limpio:\")\n",
    "df_clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSIONES DE CALIDAD DE DATOS\n",
    "\n",
    "### Diagnóstico Inicial\n",
    "\n",
    "**Dataset Original**: 920 registros, 16 variables\n",
    "\n",
    "**Problemas Identificados por Dimensión**:\n",
    "\n",
    "1. **Completitud**: 1,759 valores faltantes (19.1% del total de datos)\n",
    "   - Variables críticas con >30% de datos faltantes: ca (66.4%), thal (52.8%), slope (33.6%)\n",
    "   - Variables con datos faltantes moderados: fbs (9.8%), trestbps (6.4%), thalch (6.0%), exang (6.0%), oldpeak (6.7%), chol (3.3%), restecg (0.2%)\n",
    "\n",
    "2. **Consistencia**: Variables booleanas almacenadas como texto (TRUE/FALSE) y 173 valores anómalos de 0 en variables clínicas\n",
    "\n",
    "3. **Exactitud**: 229 valores fuera de rangos clínicos esperados, incluyendo 183 outliers en colesterol y 28 en presión arterial\n",
    "\n",
    "4. **Unicidad**: 4 registros completamente duplicados (0.43%)\n",
    "\n",
    "5. **Validez**: Tipos de datos apropiados después de conversión inicial, requiriendo solo ajustes menores en variables booleanas\n",
    "\n",
    "### Proceso de Limpieza Implementado\n",
    "\n",
    "**Paso 1 - Conversión de Tipos**: Estandarización de variables booleanas (fbs, exang) y validación de tipos numéricos\n",
    "\n",
    "**Paso 2 - Valores Anómalos**: Identificación y conversión a NaN de 173 valores de 0 en trestbps (1) y chol (172)\n",
    "\n",
    "**Paso 3 - Imputación**: Estrategia de imputación por dataset de origen con fallback a valores globales\n",
    "- Variables numéricas: mediana por dataset (trestbps, chol, thalch, oldpeak, ca)\n",
    "- Variables categóricas: moda por dataset (fbs, restecg, exang, slope, thal)\n",
    "\n",
    "**Paso 4 - Duplicados**: Eliminación de 2 registros duplicados\n",
    "\n",
    "**Paso 5 - Variable Target**: Creación de variable binaria heart_disease (0=sin enfermedad, 1=con enfermedad)\n",
    "\n",
    "### Resultados Finales\n",
    "\n",
    "**Dataset Limpio**: 918 registros, 17 variables (incluyendo variable target)\n",
    "\n",
    "**Distribución Target**:\n",
    "- Clase 0 (sin enfermedad): 410 pacientes (44.7%)\n",
    "- Clase 1 (con enfermedad): 508 pacientes (55.3%)\n",
    "\n",
    "**Calidad de Datos**:\n",
    "- Valores faltantes: 0 (100% de completitud)\n",
    "- Duplicados: 0\n",
    "- Tipos de datos: Validados y optimizados\n",
    "- Outliers: Conservados por representar casos clínicos extremos válidos\n",
    "\n",
    "**Características del Dataset Final**:\n",
    "- Edad promedio: 53.5 años (rango: 28-77)\n",
    "- Presión arterial: 132.1 ± 17.9 mmHg\n",
    "- Colesterol: 245.5 ± 55.8 mg/dl\n",
    "- Frecuencia cardíaca máxima: 136.5 ± 25.5 bpm\n",
    "- Dataset balanceado apropiado para clasificación binaria"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
